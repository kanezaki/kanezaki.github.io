
<HTML>
  <HEAD>
    <meta http-equiv="Content-Type" content="text/html; charset=Shift_JIS">
    <!--link rel="stylesheet" type="text/css" href="css/style.css" /-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
    <TITLE>Asako Kanezaki</TITLE>
  </HEAD>
  <BODY>
    <div class="container">
    <!--TABLE WIDTH="850" ALIGN="CENTER"><TR><TD-->
	  
	  <!--H1>Asako Kanezaki</H1-->

	  <a href="index_jp.html">Japanese</a>
	  
	  <UL>
	    <TABLE width="900" border="0">
	      <TR>
		<TD width="300"><IMG SRC="images/AsakoKanezakiGlasses.jpg" ALIGN=bottom name=survol1 HEIGHT=200></TD>
		<TD width="600">
		    <H2><b>Asako Kanezaki</b></H2>
		    <BR>
		    Associate Professor
		    <BR>
		    <a href="https://www.ak.c.titech.ac.jp//index-eng.html">My Lab's Website</a>
		    <BR>
		    <BR><b>Affiliation</b>
		    <BR>Tokyo Institute of Technology
		    <BR>W8-64, 2-12-1 Ookayama, Meguro-ku, Tokyo 152-8550, Japan
		</TD>
	      </TR>	    	    
	    </TABLE>
	  </UL>
	  
	  <H2>News</H2>
	  <UL>
	    <LI>Our paper "Tactile Estimation of Extrinsic Contact Patch for Stable Placement" was accepted to ICRA'24!
	    </LI>
	    <LI>Our paper "Linking Vision and Multi-Agent Communication through Visible Light Communication using Event Cameras" was accepted to AAMAS'24!
	    </LI>
	    <LI>Our paper "Egocentric Human Activities Recognition with Multi-modal Interaction Sensing" was accepted to IEEE Sensors!
	    </LI>
	    <LI>Our paper "Multi-Agent Visual Coordination using Optical Wireless Communication" was accepted to IEEE RA-L!
	    </LI>
	    <LI>Our paper "DAC: Disentanglement-and-Calibration Module for Cross-Domain Few-Shot Classification" was accepted to IEEE Access!
	    </LI>
	    <LI>We got first place in SoundSpaces Challenge in the Embodied AI Workshop in CVPR'23!
	    </LI>
	    <LI>Our paper "Multi-goal Audio-visual Navigation using Sound Direction Map" was accepted to IROS'23!
	    </LI>
	    <LI>Our paper "Point Anywhere: Directed Object Estimation from Omnidirectional Images" was accepted to SIGGRAPH'23 Poster!
	    </LI>
	    <LI>Our paper "Multi Event Localization by Audio-Visual Fusion with Omnidirectional Camera and Microphone Array" was accepted to CVPRW'23!
	    </LI>
	    <LI>Our paper "Cross-Level Distillation and Feature Denoising for Cross-Domain Few-Shot Classification" was accepted to ICLR'23!
	    </LI>
	    <LI>Our paper "H-SAUR: Hypothesize, Simulate, Act, Update, and Repeat for Understanding Object Articulations from Interactions" was accepted to ICRA'23!
	    </LI>
	    <LI>Our paper "EvIs-Kitchen: Egocentric Human Activities Recognition with Video and Inertial Sensor data" was accepted to MMM'23!
	    </LI>
	    <LI>Two papers were accepted to ICRA'22!
	    </LI>
	    <LI>Our paper "CTRMs: Learning to Construct Cooperative Timed Roadmaps for Multi-agent Path Planning in Continuous Spaces" was accepted to AAMAS'22!
	    </LI>
	    <LI>Our paper "Unsupervised Learning of Image Segmentation Based on Differentiable Feature Clustering" got the IEEE Signal Processing Society (SPS) Japan Student Journal Paper Award. Congrats Wonjik Kim!
	    </LI>
	    <LI>Our paper "Path Planning using Neural A* Search" was accepted to ICML'21!
	    </LI>
	    <LI>I was awarded the Research Encouragement Award of the Japan Society of Mechanical Engineers!
	    </LI>
	    <LI>Our paper "Unsupervised Learning of Image Segmentation Based on Differentiable Feature Clustering" got the Telecom Technology Student Award. Congrats Wonjik Kim!
	    </LI>
	    <LI>Our paper "Incremental Multi-view Object Detection from a Moving Camera" was accepted to ACM Multimedia Asia 2020!
	    </LI>
	    <LI>Our paper "Deep Reactive Planning in Dynamic Environments" was accepted to CoRL'20!
	    </LI>
	    <LI>Our paper of "Unsupervised Image Segmentation" was accepted to IEEE TIP!
	    </LI>
	    <LI>Our paper of "Efficient Exploration in Constrained Environments" was accepted to IEEE IROS'20!
	    </LI>
	    <LI>I started my own laboratory at Tokyo Institute of Technology as an associate professor!
	    </LI>
	    <LI>Our paper "Visual Object Search by Learning Spatial Context" was accepted to IEEE RA-L with ICRA'20 presentation!
	    </LI>
	    <LI>Our paper of "RotationNet" was accepted to IEEE TPAMI!
	    </LI>
	    <LI>Our paper of "Salient object detection on hyperspectral images" was accepted to IEEE ICASSP'19!
	    </LI>
	    <LI>I wrote a book chapter (Chapter 2) in <a href="https://www.elsevier.com/books/multimodal-scene-understanding/yang/978-0-12-817358-9">"Multimodal Scene Understanding: Algorithms, Applications and Deep Learning."</a>
	    </LI>
	    <LI>I gave an invited talk at <a href="http://www.ttic.edu/SNL2018/">the 2nd International Workshop on Symbolic-Neural Learning (SNL-2018)</a>.
	    </LI>
	    <LI>Our paper of "RotationNet" was accepted to IEEE CVPR'18!
	    </LI>
	    <LI>My paper of "unsupervised image segmentation" was accepted to IEEE ICASSP'18!
	    </LI>
	    <LI>Our paper of "GOSELO" was accepted to IEEE ICRA'18 presentation!
	    </LI>
	    <LI>Our paper of "GOSELO" was accepted to IEEE RA-L!
	    </LI>
	    <LI>I got the first prize at Task 1 in <a href="https://shapenet.cs.stanford.edu/shrec17/#results">the SHREC2017 Large-scale 3D Shape Retrieval from ShapeNet Core55 Challenge</a>!
	    </LI>
	    <LI>I got the first prize at <a href="http://people.sutd.edu.sg/~saikit/projects/sceneNN/shrec17/evaluation/">the SHREC2017 RGB-D Object-to-CAD Retrieval Contest</a>!
	    </LI>
	  </UL>

	  <H2>Biography</H2>
	  <UL>
	    <LI>2008 BS of Engineering (The University of Tokyo)
	    </LI>
	    <LI>2010 MA of Information Science & Technology (The University of Tokyo)
	    </LI>
	    <LI>2010 Visiting Researcher at Intelligent Autonomous Systems Group, TUM
	    </LI>
	    <LI>2013 Ph.D. of Information Science & Technology (The University of Tokyo)
	    </LI>
	    <LI>2013 Assistant Professor (The University of Tokyo)
	    </LI>
	    <LI>2016 Researcher (National Institute of Advanced Industrial Science and Technology)
	    </LI>
	    <LI>2018 Senior Researcher (National Institute of Advanced Industrial Science and Technology)
	    </LI>
	    <LI>2020 Associate Professor (Tokyo Institute of Technology)
	    </LI>
	  </UL>

	  <H2>Research</H2>
	  <UL>
	    <LI>3D Object Recognition and Detection
	    <LI>3D Shape and Color Descriptors
	    <LI>Semantic Mapping of Real Environment
	  </UL>

	  <H2>Book chapters</H2>
	  <OL>
	    <LI><U>Asako Kanezaki</U>, Ryohei Kuga, Yusuke Sugano, and Yasuyuki Matsushita (Chapter authors).
	      <BR>Chapter 2: "Multimodal Scene Understanding: Algorithms, Applications and Deep Learning."
	      <BR>Elsevier, August, 2019. (525 pages)
	      <BR><a href="https://www.elsevier.com/books/multimodal-scene-understanding/yang/978-0-12-817358-9">LINK</a>
	      <BR>
	  </OL>

	  <H2>Journal publications</H2>
	  <OL>
	    <LI>Yuzhe Hao, <U>Asako Kanezaki</U>, Ikuro Sato, Rei Kawakami, and Koichi Shinoda.
	      <BR>Egocentric Human Activities Recognition with Multi-modal Interaction Sensing.
	      <BR><I>IEEE Sensors</I>, accepted, 2024.
	      <BR><BR>

	    <LI>Haruyuki Nakagawa and <U>Asako Kanezaki</U>.
	      <BR>Multi-Agent Visual Coordination using Optical Wireless Communication.
	      <BR><I>IEEE Robotics and Automation Letters (RA-L)</I>, accepted, 2023.
	      <BR><a href="https://doi.org/10.1109/LRA.2023.3304905">https://doi.org/10.1109/LRA.2023.3304905</a>
	      <BR><BR>

	    <LI>Hao Zheng, Qiang Zhang, and <U>Asako Kanezaki</U>.
	      <BR>DAC: Disentanglement-and-Calibration Module for Cross-Domain Few-Shot Classification.
	      <BR><I>IEEE Access</I>, Vol. 11, No. 23552466, pp. 82665-82673, 2023.
	      <BR><a href="https://doi.org/10.1109/ACCESS.2023.3294984">https://doi.org/10.1109/ACCESS.2023.3294984</a>
	      <BR><BR>

	    <LI><U>Asako Kanezaki</U>, Yasuyuki Matsushita, and Yoshifumi Nishida.
	      <BR>RotationNet for Joint Object Categorization and Unsupervised Pose Estimation from Multi-view Images.
	      <BR><I> IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</I>, Vol.43, Issue 1, pp. 269-283, 2021.
	      <BR><a href="https://doi.org/10.1109/TPAMI.2019.2922640">https://doi.org/10.1109/TPAMI.2019.2922640</a>
	      &nbsp;
	      <BR><a href="https://kanezaki.github.io/rotationnet/">Project</a>
	      <BR><BR>

	    <LI>Wonjik Kim*, <U>Asako Kanezaki</U>*, and Masayuki Tanaka.
	      <BR>Unsupervised Learning of Image Segmentation Based on Differentiable Feature Clustering.
	      <BR><I>IEEE Transactions on Image Processing</I>, pp. 8055-8068, 2020.
	      <BR><b>*equal contributions</b>
	      <BR><a href="https://kanezaki.github.io/pytorch-unsupervised-segmentation-tip/">Project</a>
	      <BR><BR>
	    </LI>
	    
	    <LI>Raphael Druon, Yusuke Yoshiyasu, <U>Asako Kanezaki</U>, and Alassane Watt.
	      <BR>Visual Object Search by Learning Spatial Context.
	      <BR><I>IEEE Robotics and Automation Letters (RA-L)</I>, Vol.5, Issue 2, pp. 1279-1286, 2020. (<b>presented in ICRA'20</b>)
	      <BR><a href="https://doi.org/10.1109/LRA.2020.2967677">https://doi.org/10.1109/LRA.2020.2967677</a>
	      <BR><BR>

	    <LI><U>Asako Kanezaki</U>, Jirou Nitta, and Yoko Sasaki.
	      <BR>GOSELO: Goal-Directed Obstacle and Self-Location Map for Robot Navigation using Reactive Neural Networks.
	      <BR><I>IEEE Robotics and Automation Letters (RA-L)</I>, Vol.3, Issue 2, pp. 696-703, 2018. (<b>presented in ICRA'18</b>)
	      <BR><a href="https://doi.org/10.1109/LRA.2017.2783400">https://doi.org/10.1109/LRA.2017.2783400</a>
	      &nbsp;
	      <BR><a href="https://kanezaki.github.io/goselo/">Project</a>
	      <BR><BR>

	    <LI>Nevrez Imamoglu, Wataru Shimoda, Chi Zhang, Yuming Fang, <U>Asako Kanezaki</U>, Keiji Yanai, and Yoshifumi Nishida.
	      <BR>An Integration of Bottom-up and Top-Down Salient Cues on RGB-D Data: Saliency from Objectness vs. Non-Objectness.
	      <BR><I>Signal, Image and Video Processing (Springer)</I>, Vol.12, Issue 2, pp. 307-314, 2017.
	      <BR><a href="https://doi.org/10.1007/s11760-017-1159-7">https://doi.org/10.1007/s11760-017-1159-7</a>
	      <BR><BR>

	    <LI>Zoltan-Csaba Marton, Ferenc Balint-Benczedi, Oscar Martinez Mozos, Nico Blodow, <U>Asako Kanezaki</U>, Lucian Cosmin Goron, Dejan Pangercic, and Michael Beetz.
	      <BR>Part-Based Geometric Categorization and Object Reconstruction in Cluttered Table-Top Scenes. 
	      <BR><I>Journal of Intelligent & Robotic Systems</I>, pp. 35-56, 2014. 
	      <BR><a href="http://dx.doi.org/10.1007/s10846-013-0011-8">http://dx.doi.org/10.1007/s10846-013-0011-8</a>
	      <BR><BR>

	    <LI><U>Asako Kanezaki</U>, Tatsuya Harada, and Yasuo Kuniyoshi.
	      <BR>Partial Matching of Real Textured 3D Objects Using Color Cubic Higher-order Local Auto-Correlation Features. 
	      <BR><I>The Visual Computer</I>, Vol.26, No.10, pp. 1269-1281, 2010.
	      <BR><a href="https://doi.org/10.1007/s00371-010-0521-3">https://doi.org/10.1007/s00371-010-0521-3</a>
	      <BR><BR>

	    <LI>Tatsuya Harada, <U>Asako Kanezaki</U>, and Yasuo Kuniyoshi.
	      <BR>The Development of Color CHLAC Features for Object Exploration in 3D Map. 
	      <BR><I>Journal of the Robotics Society of Japan</I>, Vol.27, No.7, pp. 749-758, 2009. (in Japanese)
	      <!--BR><a href="http://www.i-product.biz/rsj/Abs/Vol_27/No_07/2707A047.html">Paper</a-->
	      <BR>
    
	  </OL>

	  <H2>International Conference</H2>
	  <OL>
	    <LI>Kei Ota, Devesh Jha, Krishna Murthy Jatavallabhula, <U>Asako Kanezaki</U>, and Joshua B. Tenenbaum.
	      <BR>Tactile Estimation of Extrinsic Contact Patch for Stable Placement.
	      <BR><I>IEEE International Conference on Robotics and Automation (ICRA)</I>, accepted, 2024.
	      <BR><BR>

	    <LI>Haruyuki Nakagawa, Yoshitaka Miyatani, and <U>Asako Kanezaki</U>.
	      <BR>Linking Vision and Multi-Agent Communication through Visible Light Communication using Event Cameras.
	      <BR><I>Int. Joint Conf. on Autonomous Agents & Multiagent Systems (AAMAS)</I>, accepted, 2024.
	      <BR><a href="https://arxiv.org/abs/2402.05619">Paper</a>
	      <BR><BR>

	    <LI>Haru Kondoh and <U>Asako Kanezaki</U>.
	      <BR>Multi-goal Audio-visual Navigation using Sound Direction Map.
	      <BR><I>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</I>, accepted, 2023.
	      <BR><a href="https://arxiv.org/abs/2308.00219">Paper</a>
	      <BR><BR>

	    <LI>Nanami Kotani and <U>Asako Kanezaki</U>.
	      <BR>Point Anywhere: Directed Object Estimation from Omnidirectional Images.
	      <BR><I>Special Interest Group on Computer Graphics and Interactive Techniques Conference Posters (SIGGRAPH '23 Posters)</I>, accepted, 2023.
	      <BR><a href="https://arxiv.org/abs/2308.01010">Paper</a>
	      &nbsp;
	      <a href="https://github.com/NKotani/PointAnywhere">Dataset</a>
	      <BR><BR>

	    <LI>Wenru Zheng, Ryota Yoshihashi, Rei Kawakami, Ikuro Sato, and <U>Asako Kanezaki</U>.
	      <BR>Multi Event Localization by Audio-Visual Fusion with Omnidirectional Camera and Microphone Array.
	      <BR><I>6th CVPR Workshop on Multimodal Learning and Applications (MULA)</I>, accepted, 2023.
	      <BR><a href="https://github.com/zwr17/MLMC-AVE">Dataset</a>
	      <BR><BR>

	    <LI>Hao Zheng, Runqi Wang, Jianzhuang Liu, and <U>Asako Kanezaki</U>.
	      <BR>Cross-Level Distillation and Feature Denoising for Cross-Domain Few-Shot Classification.
	      <BR><I>International Conference on Learning Representations (ICLR)</I>, accepted, 2023.
	      <BR><a href="https://openreview.net/forum?id=Kn-HA8DFik">Paper</a>
	      &nbsp;
	      <a href="https://github.com/jarucezh/cldfd">Code</a>
	      <BR><BR>

	    <LI>Kei Ota, Hsiao-Yu Tung, Kevin A. Smith, Anoop Cherian, Tim K. Marks, Alan Sullivan, <U>Asako Kanezaki</U>, and Joshua B. Tenenbaum.
	      <BR>H-SAUR: Hypothesize, Simulate, Act, Update, and Repeat for Understanding Object Articulations from Interactions.
	      <BR><I>IEEE International Conference on Robotics and Automation (ICRA)</I>, accepted, 2023.
	      <BR><a href="https://arxiv.org/abs/2210.12521">Paper</a>
	      <BR><BR>

	    <LI>Yuzhe Hao, Kuniaki Uto, <U>Asako Kanezaki</U>, Ikuro Sato, Rei Kawakami, and Koichi Shinoda.
	      <BR>EvIs-Kitchen: Egocentric Human Activities Recognition with Video and Inertial Sensor data.
	      <BR><I>Multimedia Modeling (MMM)</I>, accepted, 2023.
	      <BR><BR>

	    <LI>Hana Hoshino, Kei Ota, <U>Asako Kanezaki</U>, and Rio Yokota.
	      <BR>OPIRL: Sample Efficient Off-Policy Inverse Reinforcement Learning via Distribution Matching.
	      <BR><I>IEEE International Conference on Robotics and Automation (ICRA)</I>, accepted, 2022.
	      <BR><a href="https://arxiv.org/abs/2109.04307">Paper</a>
	      &nbsp;
	      <a href="https://github.com/sff1019/opirl">Code</a>
	      <BR><BR>

	    <LI>Rui Fukushima, Kei Ota, <U>Asako Kanezaki</U>, Yoko Sasaki, and Yusuke Yoshiyasu.
	      <BR>Object Memory Transformer for Object Goal Navigation.
	      <BR><I>IEEE International Conference on Robotics and Automation (ICRA)</I>, accepted, 2022.
	      <BR><a href="https://arxiv.org/abs/2203.14708">Paper</a>
	      &nbsp;
	      <a href="https://github.com/TetsuyaMurata/target-driven-navigation-based-on-transformer">Code</a>
	      <BR><BR>

	    <LI>Keisuke Okumura, Ryo Yonetani, Mai Nishimura, and <U>Asako Kanezaki</U>.
	      <BR>CTRMs: Learning to Construct Cooperative Timed Roadmaps for Multi-agent Path Planning in Continuous Spaces.
	      <BR><I>Int. Joint Conf. on Autonomous Agents & Multiagent Systems (AAMAS)</I>, accepted, 2022.
	      <BR><a href="https://omron-sinicx.github.io/ctrm/">Project</a>
	      &nbsp;
	      <a href="https://arxiv.org/abs/2201.09467">Paper</a>
	      &nbsp;
	      <a href="https://github.com/omron-sinicx/ctrm/">Code</a>
	      <BR><BR>

	    <LI>Ryo Yonetani, Tatsunori Taniai, Mohammadamin Barekatain, Mai Nishimura, and <U>Asako Kanezaki</U>.
	      <BR>Path Planning using Neural A* Search.
	      <BR><I>International Conference on Machine Learning (ICML)</I>, pp. 12029-12039, 2021.
	      <BR><a href="https://omron-sinicx.github.io/neural-astar/">Project</a>
	      &nbsp;
	      <a href="https://arxiv.org/abs/2009.07476">Paper</a>
	      <BR><BR>

	    <LI>Takashi Konno, Ayako Amma, and <U>Asako Kanezaki</U>.
	      <BR>Incremental Multi-view Object Detection from a Moving Camera.
	      <BR><I>ACM Multimedia Asia (ACMMMA)</I>, pp. 1-7, 2020.
	      <BR><a href="https://www.ak.c.titech.ac.jp/projects/IMOD/">Project</a>
	      <BR><BR>

	    <LI>Kei Ota, Devesh K. Jha, Tadashi Onishi, <U>Asako Kanezaki</U>, Yusuke Yoshiyasu, Yoko Sasaki, Toshisada Mariyama, Daniel Nikovski.
	      <BR>Deep Reactive Planning in Dynamic Environments.
	      <BR><I>The Conference on Robot Learning (CoRL)</I>, 2020.
	      <BR><a href="https://arxiv.org/abs/2011.00155">Paper</a>
	      &nbsp;
	      <a href="https://www.youtube.com/watch?v=hE-Ew59GRPQ&feature=youtu.be">YouTube</a>
	      <BR><BR>

	    <LI>Kei Ota, Yoko Sasaki, Devesh K. Jha, Yusuke Yoshiyasu, and <U>Asako Kanezaki</U>.
	      <BR>Efficient Exploration in Constrained Environments with Goal-Oriented Reference Path.
	      <BR><I>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2020)</I>, pp. 6061-6068, 2020.
	      <BR><a href="https://arxiv.org/abs/2003.01641">Paper</a>
	      <BR><BR>

	    <LI>Yoko Sasaki, Syusuke Matsuo, <U>Asako Kanezaki</U>, and Hiroshi Takemura.
	      <BR>A3C Based Motion Learning for an Autonomous Mobile Robot in Crowds.
	      <BR><I>IEEE International Conference on System Man and Cybernetics (SMC2019)</I>, pp. 1046-1052, 2019.
	      <BR><BR>

	    <LI>Nevrez Imamoglu, Guanqun Ding, Yuming Fang, <U>Asako Kanezaki</U>, Toru Kouyama, and Ryosuke Nakamura.
	      <BR>Salient object detection on hyperspectral images using features learned from unsupervised segmentation task.
	      <BR><I>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</I>, pp. 2192-2196, 2019.
	      <BR><a href="https://arxiv.org/abs/1902.10993">Paper</a>
	      <BR><BR>

	    <LI><U>Asako Kanezaki</U>, Yasuyuki Matsushita, and Yoshifumi Nishida.
	      <BR>RotationNet: Joint Object Categorization and Pose Estimation Using Multiviews from Unsupervised Viewpoints.
	      <BR><I>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</I>, pp. 5010-5019, 2018.
	      <BR><a href="https://kanezaki.github.io/rotationnet/">Project</a>
	      <BR><BR>

	    <LI><U>Asako Kanezaki</U>.
	      <BR>Unsupervised Image Segmentation by Backpropagation.
	      <BR><I>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</I>, pp. 1543-1547, 2018.
	      <BR><a href="https://kanezaki.github.io/pytorch-unsupervised-segmentation-tip/">Project</a>
	      <BR><BR>

	    <LI>Mikiko Oono, Yoshifumi Nishida, Koji Kitamura, <U>Asako Kanezaki</U>, and Tatsuhiro Yamanaka.
	      <BR>"Change the changeable" framework for implementation research in health.
	      <BR><I>the 10th International Conference on Computer Supported Education (CSEDU)</I>, (oral), 2017.
	      <BR><BR>

	    <LI>Ryohei Kuga, <U>Asako Kanezaki</U>, Masaki Samejima, Yusuke Sugano, and Yasuyuki Matsushita.
	      <BR>Multi-task Learning Using Multi-modal Encoder-Decoder Networks with Shared Skip Connections.
	      <BR><I>IEEE/ISPRS 4th Joint Workshop on Multi-Sensor Fusion for Dynamic Scene Understanding</I>, pp. 403-411, (oral), 2017.
	      <BR><a href="https://doi.org/10.1109/ICCVW.2017.54">https://doi.org/10.1109/ICCVW.2017.54</a>
	      &nbsp;
	      <BR><a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w6/Kuga_Multi-Task_Learning_Using_ICCV_2017_paper.pdf">Paper</a>
	      <BR><BR>

	    <LI>Manolis Savva, Fisher Yu, Hao Su, <U>Asako Kanezaki</U>, Takahiko Furuya, Ryutarou Ohbuchi, Zhichao Zhou, Rui Yu, Song Bai, Xiang Bai, Masaki Aono, Atsushi Tatsuma, S. Thermos, A. Axenopoulos, G. Th. Papadopoulos, P. Daras, Xiao Deng, Zhouhui Lian, Bo Li, Henry Johan, Yijuan Lu, and Sanjeev Mk.
	      <BR>SHREC'17 Track: Large-Scale 3D Shape Retrieval from ShapeNet Core55.
	      <BR><I>Eurographics Workshop on 3D Object Retrieval 2017</I>, 2017.
	      <BR><a href="https://shapenet.cs.stanford.edu/shrec17/shrec17shapenet.pdf">Paper</a>
	      <BR><BR>

	    <LI>Binh-Son Hua, Quang-Trung Truong, Minh-Khoi Tran, Quang-Hieu Pham, <U>Asako Kanezaki</U>, Tang Lee, HungYueh Chiang, Winston Hsu, Bo Li, Yijuan Lu, Henry Johan, Shoki Tashiro, Masaki Aono, Minh-Triet Tran, Viet-Khoi Pham, Hai-Dang Nguyen, Vinh-Tiep Nguyen, Quang-Thang Tran, Thuyen V. Phan, Bao Truong, Minh N. Do, Anh-Duc Duong, Lap-Fai Yu, Duc Thanh Nguyen, and Sai-Kit Yeung.
	      <BR>SHREC'17: RGB-D to CAD Retrieval with ObjectNN Dataset.
	      <BR><I>Eurographics Workshop on 3D Object Retrieval 2017</I>, 2017.
	      <BR><a href="http://sonhua.github.io/pdf/hua-shrec17.pdf">Paper</a>
	      <BR><BR>

	    <LI>Tomoaki Saito, <U>Asako Kanezaki</U>, and Tatsuya Harada.
	      <BR>IBC127: Video Dataset for Fine-grained Bird Classification.
	      <BR><I>IEEE International Conference on Multimedia & Expo (ICME)</I>, 2016.
	      <BR><a href="http://www.mi.t.u-tokyo.ac.jp/projects/IBC127/">Dataset</a>
	      <BR><BR>

	    <LI>Katsunori Ohnishi, Atsushi Kanehira, <U>Asako Kanezaki</U>, and Tatsuya Harada.
	      <BR>Recognizing Activities of Daily Living with a Wrist-mounted Camera.
	      <BR><I>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</I>, (spotlight), 2016.
	      <BR><a href="http://www.mi.t.u-tokyo.ac.jp/static/projects/miladl/">Project</a>
	      <BR><BR>

	    <LI>Chie Kamada, <U>Asako Kanezaki</U>, and Tatsuya Harada.
	      <BR>Probabilistic Semi-Canonical Correlation Analysis.
	      <BR><I>the 23rd Annual ACM International Conference on Multimedia (ACMMM 2015)</I>, pp. 1131-1134, 2015.
	      <BR><BR>

	    <LI><U>Asako Kanezaki</U> and Tatsuya Harada.
	      <BR>3D Selective Search for Obtaining Object Candidates.
	      <BR><I>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2015)</I>, pp. 82-87, 2015.
	      <BR><a href="http://www.mi.t.u-tokyo.ac.jp/kanezaki/pdf/IROS2015_kanezaki.pdf">Paper</a>
	      &nbsp;
	      <a href="https://github.com/kanezaki/selective_search_3d">Code</a>
	      <BR><BR>

	    <LI><U>Asako Kanezaki</U>, Emanuele Rodol&agrave, Daniel Cremers, and Tatsuya Harada.
	      <BR>Learning Similarities for Rigid and Non-Rigid Object Detection.
	      <BR><I>International Conference on 3D Vision (3DV 2014)</I>, pp. 720-727, 2014.
	      <BR><a href="https://vision.in.tum.de/_media/spezial/bib/rodola-3dv14.pdf">Paper</a>
	      <BR><BR>

	    <LI>Sho Inaba, <U>Asako Kanezaki</U>, and Tatsuya Harada.
	      <BR>Automatic Image Synthesis from Keywords Using Scene Context.
	      <BR><I>the 22nd Annual ACM International Conference on Multimedia (ACMMM 2014)</I>, pp. 1149-1152, 2014.
	      <BR><BR>

	    <LI>Masaru Mizuochi, <U>Asako Kanezaki</U>, and Tatsuya Harada.
	      <BR>Clothing Retrieval Based on Local Similarity with Multiple Images.
	      <BR><I>the 22nd Annual ACM International Conference on Multimedia (ACMMM 2014)</I>, pp. 1165-1168, 2014.
	      <BR><BR>

	    <LI><U>Asako Kanezaki</U>, Yusuke Mukuta, and Tatsuya Harada.
	      <BR>Mirror Reflection Invariant HOG descriptors for Object Detection.
	      <BR><I>IEEE 21st International Conference on Image Processing (ICIP 2014)</I>, pp. 1594-1598, 2014.
	      <BR><a href="https://ieeexplore.ieee.org/abstract/document/7025319/">Paper</a>
	      <BR><BR>

	    <LI><U>Asako Kanezaki</U>, Sho Inaba, Yoshitaka Ushiku, Yuya Yamashita, Hiroshi Muraoka, Yasuo Kuniyoshi, and Tatsuya Harada.
	      <BR>Hard Negative Classes for Multiple Object Detection.
	      <BR><I>IEEE International Conference on Robotics and Automation (ICRA 2014)</I>, pp. 3066-3073, 2014.
	      <BR><a href="https://ieeexplore.ieee.org/document/6907300/">Paper</a>
	      <BR><BR>

	    <LI><U>Asako Kanezaki</U>, Yasuo Kuniyoshi, and Tatsuya Harada.
	      <BR>Weakly-supervised Multi-class Object Detection Using Multi-type 3D Features.
	      <BR><I>the 21st Annual ACM International Conference on Multimedia (ACMMM 2013)</I>, pp. 605-608, 2013.
	      <BR><a href="http://www.isi.imi.i.u-tokyo.ac.jp/~harada/pdf/acmmm2013_kanezaki_harada.pdf">Paper</a>
	      &nbsp;
	      <a href="http://www.mi.t.u-tokyo.ac.jp/kanezaki/color_depth_dataset_100.html">Dataset</a>
	      <BR><BR>

	    <LI><U>Asako Kanezaki</U>, Tatsuya Harada, and Yasuo Kuniyoshi.
	      <BR>Scale and Rotation Invariant Color Features for Weakly-Supervised Object Learning in 3D Space.
	      <BR><I>IEEE ICCV Workshop on 3D Representation and Recognition (3dRR-11)</I>, 2011.
	      <BR><a href="http://www.isi.imi.i.u-tokyo.ac.jp/~kanezaki/publications/iccv2011_3dRR_kanezaki.pdf">Paper</a>
	      <BR><BR>

	    <LI><U>Asako Kanezaki</U>, Zoltan-Csaba Marton, Dejan Pangercic, Tatsuya Harada, Yasuo Kuniyoshi, and Michael Beetz.
	      <BR>Voxelized Shape and Color Histograms for RGB-D.
	      <BR><I>IEEE IROS Workshop on Active Semantic Perception and Object Search in the Real World (ASP-AVS-11)</I>, 2011.
	      <BR><a href="https://pdfs.semanticscholar.org/3a23/b78ca2875afa0ecea6a7fdbd997c03f71a8f.pdf">Paper</a>
	      <BR><BR>

	    <LI><U>Asako Kanezaki</U>, Takahiro Suzuki, Tatsuya Harada, and Yasuo Kuniyoshi.
	      <BR>Fast Object Detection for Robots in a Cluttered Indoor Environment Using Integral 3D Feature Table. 
	      <BR><I>IEEE International Conference on Robotics and Automation (ICRA 2011)</I>, pp. 4026-4033, 2011.
	      <BR><a href="http://www.isi.imi.i.u-tokyo.ac.jp/~kanezaki/publications/icra2011_kanezaki.pdf">Paper</a>
	      <BR><BR>

	    <LI><U>Asako Kanezaki</U>, Hideki Nakayama, Tatsuya Harada, and Yasuo Kuniyoshi.
	      <BR>High-speed 3D Object Recognition Using Additive Features in A linear Subspace. 
	      <BR><I>2010 IEEE International Conference on Robotics and Automation (ICRA 2010)</I>, pp. 3128-3134, 2010.
	      <BR><font color="#ff0000">2010 IEEE Robotics and Automation Society Japan Chapter Young Award (ICRA 2010)</font>
	      <BR><a href="http://www.isi.imi.i.u-tokyo.ac.jp/~kanezaki/publications/icra2010_kanezaki.pdf">Paper</a>
	      <BR><BR>

	    <LI><U>Asako Kanezaki</U>, Tatsuya Harada, and Yasuo Kuniyoshi.
	      <BR>Partial Matching for Real Textured 3D Objects using Color Cubic Higher-order Local Auto-Correlation Features.
	      <BR><I>Eurographics Workshop on 3D Object Retrieval 2009</I>, pp. 9-12, 2009.
	      <BR><BR>

	  </OL>

	  <H2>Awards</H2>
	  <UL>
	    <LI>2020 The Research Encouragement Award of the Japan Society of Mechanical Engineers.
	    <LI>2019 The 22nd Meeting on Image Recognition and Understanding (MIRU2019) Outstanding Reviewer Award.
	    <LI>2017 <U>First place</U> in Task 1 in SHREC 2017: Large-scale 3D Shape Retrieval from ShapeNet Core55 Challenge.
	    <LI>2017 <U>First place</U> in SHREC 2017: RGB-D Object-to-CAD Retrieval Contest.
	    <LI>2015 RSJ Young Investigation Excellence Award.
	    <LI>2014 Funai Foundation for Information Technology (FFIT) Research Encouragement Award.
	    <LI>2012 Pattern Recognition and Media Understanding (PRMU) Research Encouragement Award.
	    <LI>2011 <U>Third place</U> in the classification task, <U>second place</U> in the detection task. Large Scale Visual Recognition Challenge 2011 (ILSVRC2011).
	    <LI>2010 IEEE Robotics and Automation Society Japan Chapter Young Award (ICRA 2010).
	  </UL>
	  	  
	  <H2>Grants/Fellowships</H2>
	  <UL>
	    <LI>2021-2028 JST Fusion Oriented Research for Disruptive Science and Technology (FOREST)
	    <LI>2020 Grant of Kayamori Foundation of Informational Science Advancement
	    <LI>2020-2023 Japan Society for the Promotion of Science (JSPS) Grant-in-Aid for Early-Career Scientists
	    <LI>2016-2019 Japan Society for the Promotion of Science (JSPS) Grant-in-Aid for Challenging Exploratory Research
	    <LI>2015 Microsoft Research sponsors collaborative research (CORE) project
	    <LI>2014-2016 Japan Society for the Promotion of Science (JSPS) Grant-in-Aid for Research Activity Start-up
	    <LI>2010-2013 Japan Society for the Promotion of Science (JSPS) Research Fellow (DC1)
	  </UL>

	  <BR>
	  
	  <H2>Contact</H2>
	  <UL>
	    <LI>e-mail: kanezaki[at]c.titech.ac.jp
	  </UL>
	  	  
    <!--/TD></TR></TABLE-->
    
</UL>

</div>

<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

</BODY>
</HTML>

