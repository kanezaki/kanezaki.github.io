
<HTML>
  <HEAD>
    <meta http-equiv="Content-Type" content="text/html; charset=Shift_JIS">
    <!--link rel="stylesheet" type="text/css" href="css/style.css" /-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
    <TITLE>金崎 朝子</TITLE>
  </HEAD>
  <BODY>
    <div class="container">
    <!--TABLE WIDTH="850" ALIGN="CENTER"><TR><TD-->
	  
	  <!--H1>Asako Kanezaki</H1-->

	  <a href="index.html">English</a>
	  
	  <UL>
	    <TABLE width="950" border="0">
	      <TR>
		<TD width="300"><IMG SRC="images/AsakoKanezakiGlasses.jpg" ALIGN=bottom name=survol1 HEIGHT=200></TD>
		<TD width="650">
		    <H2><b>金崎 朝子</b></H2>
		    <BR>
		    准教授 / 理化学研究所 革新知能統合研究センター (AIP) チームディレクター
		    <BR>
		    <a href="https://www.ak.c.titech.ac.jp/">研究室ホームページ</a>
		    <BR>
		    <BR><b>Affiliation</b>
		    <BR>東京科学大学（旧・東京工業大学）
		    <BR>〒152-8550 東京都目黒区大岡山2-12-1 W8-64 西8号館5階E502
		</TD>
	      </TR>	    	    
	    </TABLE>
	  </UL>
	  
	  <H2>News</H2>
	  <UL>
	    <LI>我々の論文がIROS'25にアクセプトされました。
	    </LI>
	    <LI>2本の論文がMVA'25にアクセプトされました。
	    </LI>
	    <LI>我々の論文がThe 6th Embodied AI workshop at CVPR 2025にアクセプトされました。
	    </LI>
	    <LI>我々の論文がECCV'24のオーラル発表にアクセプトされました。
	    </LI>
	    <LI>論文"説明文生成を補助タスクとする意味的視聴覚ナビゲーション"がMIRU学生優秀賞を受賞しました。近藤暖さんおめでとう！
	    </LI>
	    <LI>4本の論文をMIRU 2024にて発表しました。
	    </LI>
	    <LI>2本の論文をICRA'24 Workshopで発表しました。
	    </LI>
	    <LI>我々の論文がMachine Learningにアクセプトされました。
	    </LI>
	    <LI>我々の論文がICRA'24にアクセプトされました。
	    </LI>
	    <LI>我々の論文がAAMAS'24にアクセプトされました。
	    </LI>
	    <LI>我々の論文がIEEE Sensorsにアクセプトされました。
	    </LI>
	    <LI>金崎がプロジェクトマネージャーを務める<a href="https://kakusei.aist.go.jp/">覚醒プロジェクト</a>の応募が始まりました。
	    </LI>
	    <LI>我々の論文がIEEE RA-Lにアクセプトされました。
	    </LI>
	    <LI>我々の論文がIEEE Accessにアクセプトされました。
	    </LI>
	    <LI>CVPR'23のEmbodied AI WorkshopのSoundSpaces Challengeで1位を獲得しました。
	    </LI>
	    <LI>我々の論文がIROS'23にアクセプトされました。
	    </LI>
	    <LI>我々の論文がSIGGRAPH'23 Posterにアクセプトされました。
	    </LI>
	    <LI>我々の論文がCVPRW'23にアクセプトされました。
	    </LI>
	    <LI>我々の論文がICLR'23にアクセプトされました。
	    </LI>
	    <LI>我々の論文がICRA'23にアクセプトされました。
	    </LI>
	    <LI>我々の論文がMMM'23にアクセプトされました。
	    </LI>
	    <LI>著書（共著）<a href="https://www.amazon.co.jp/%E8%A9%B3%E8%A7%A3-3%E6%AC%A1%E5%85%83%E7%82%B9%E7%BE%A4%E5%87%A6%E7%90%86-Python%E3%81%AB%E3%82%88%E3%82%8B%E5%9F%BA%E7%A4%8E%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0%E3%81%AE%E5%AE%9F%E8%A3%85-KS%E7%90%86%E5%B7%A5%E5%AD%A6%E5%B0%82%E9%96%80%E6%9B%B8-%E9%87%91%E5%B4%8E/dp/406529343X/?_encoding=UTF8&pd_rd_w=rKXEn&content-id=amzn1.sym.79ec8dbb-a6ed-4b2c-b8a2-e2dc98848cba&pf_rd_p=79ec8dbb-a6ed-4b2c-b8a2-e2dc98848cba&pf_rd_r=VEHX8P7YH5CV32BYK1P4&pd_rd_wg=xkb0h&pd_rd_r=7f742698-b561-4161-a452-e938c2f2d199&ref_=pd_gw_ci_mcx_mr_hp_atf_m">『詳解 3次元点群処理 Pythonによる基礎アルゴリズムの実装』</a>が出版されました。
	    </LI>
	    <LI>第40回 日本ロボット学会学術講演会にて招待講演を行いました。
	    </LI>
	    <LI>4本の論文をMIRU 2022にて発表しました。
	    </LI>
	    <LI>2本の論文がICRA'22にアクセプトされました。
	    </LI>
	    <LI>論文"CTRMs: Learning to Construct Cooperative Timed Roadmaps for Multi-agent Path Planning in Continuous Spaces"がAAMAS'22にアクセプトされました。
	    </LI>
	    <LI>論文"Unsupervised Learning of Image Segmentation Based on Differentiable Feature Clustering"がIEEE Signal Processing Society (SPS) Japan Student Journal Paper Awardを受賞しました。Wonjik Kimさんおめでとう！
	    </LI>
	    <LI>論文"Path Planning using Neural A* Search"がICML'21にアクセプトされました。
	    </LI>
	    <LI>日本機械学会奨励賞（研究）を受賞しました。
	    </LI>
	    <LI>論文"Unsupervised Learning of Image Segmentation Based on Differentiable Feature Clustering"がテレコム技術学生賞を受賞しました。Wonjik Kimさんおめでとう！
	    </LI>
	    <LI>論文"Incremental Multi-view Object Detection from a Moving Camera"がACM Multimedia Asia 2020にアクセプトされました。
	    </LI>
	    <LI>論文"Deep Reactive Planning in Dynamic Environments"がCoRL'20にアクセプトされました。
	    </LI>
	    <LI>論文"Unsupervised Image Segmentation"がIEEE TIPにアクセプトされました。
	    </LI>
	    <LI>論文"Efficient Exploration in Constrained Environments"がIEEE IROS'20にアクセプトされました。
	    </LI>
	    <LI>東京工業大学の准教授として研究室を立ち上げました。
	    </LI>
	    <LI>論文"Visual Object Search by Learning Spatial Context"がIEEE RA-L with ICRA'20 presentationにアクセプトされました。
	    </LI>
	    <LI>論文"RotationNet"がIEEE TPAMIにアクセプトされました。
	    </LI>
	    <LI>論文"Salient object detection on hyperspectral images"がIEEE ICASSP'19にアクセプトされました。
	    </LI>
	    <LI>Elsevier出版<a href="https://www.elsevier.com/books/multimodal-scene-understanding/yang/978-0-12-817358-9">"Multimodal Scene Understanding: Algorithms, Applications and Deep Learning"</a>の第2章を執筆しました。
	    </LI>
	    <LI><a href="http://www.ttic.edu/SNL2018/">Second International Workshop on Symbolic-Neural Learning (SNL-2018)</a>にて招待講演を行いました。
	    </LI>
	    <LI>論文"RotationNet"がIEEE CVPR'18にアクセプトされました。
	    </LI>
	    <LI>論文"unsupervised image segmentation"がIEEE ICASSP'18にアクセプトされました。
	    </LI>
	    <LI>論文"GOSELO"がIEEE ICRA'18プレゼンテーションにアクセプトされました。
	    </LI>
	    <LI>論文"GOSELO"がIEEE RA-Lジャーナルにアクセプトされました。
	    </LI>
	    <LI>三次元物体検索の国際的コンペティション（SHREC2017）において、<a href="https://shapenet.cs.stanford.edu/shrec17/#results">大規模CADモデル検索部門</a>のタスク１で第一位を獲得しました。
	    </LI>
	    <LI>三次元物体検索の国際的コンペティション（SHREC2017）において、<a href="http://people.sutd.edu.sg/~saikit/projects/sceneNN/shrec17/evaluation/">RGBDデータからの物体のCADモデル検索部門</a>で第一位を獲得しました。
	    </LI>
	  </UL>

	  <H2>Biography</H2>
	  <UL>
	    <LI>2008 BS of Engineering (The University of Tokyo)
	    </LI>
	    <LI>2010 MA of Information Science & Technology (The University of Tokyo)
	    </LI>
	    <LI>2010 Visiting Researcher at Intelligent Autonomous Systems Group, TUM
	    </LI>
	    <LI>2013 Ph.D. of Information Science & Technology (The University of Tokyo)
	    </LI>
	    <LI>2013 Assistant Professor (The University of Tokyo)
	    </LI>
	    <LI>2016 Researcher (National Institute of Advanced Industrial Science and Technology)
	    </LI>
	    <LI>2018 Senior Researcher (National Institute of Advanced Industrial Science and Technology)
	    </LI>
	    <LI>2020 Associate Professor (Tokyo Institute of Technology)
	    </LI>
	  </UL>

	  <H2>Research</H2>
	  <UL>
	    <LI>3D Object Recognition and Detection
	    <LI>3D Shape and Color Descriptors
	    <LI>Semantic Mapping of Real Environment
	  </UL>


	  <H2>資料</H2>
	  <UL>
	    <LI>2024.09.28 映像情報メディア学会誌 Vol. 79, No. 1, pp. 60～61（2025）きらり。中のヒト 第19回<a href="https://kanezaki.github.io/media/2501%E3%81%8D%E3%82%89%E3%82%8A.pdf">「わたしの大好きなロボットたち」</a> </LI>
	    <LI>2022.09.06  第40回 日本ロボット学会学術講演会 確率ロボティクスとデータ工学ロボティクス～認識・行動学習・記号創発～ <a href="https://kanezaki.github.io/media/20220906_%E3%83%AD%E3%83%9C%E3%83%83%E3%83%88%E5%AD%A6%E4%BC%9AOS%E8%AC%9B%E6%BC%94_25min.pdf">深層学習を用いたロボットナビゲーション</a> </LI>
	    <LI>2020.11.02 情報処理 Vol.62 No.2 Feb. 2021 連載 5 分で分かる！？有名論文ナナメ読み <a href="https://kanezaki.github.io/media/6202naname-2.pdf">"Zhang, Qi and Goldman, Sally A. EM-DD: An Improved Multiple-Instance Learning Technique"</a> </LI>
	    <LI>2020.04.25  東京工業大学 情報理工学院 入試説明会<a href="https://kanezaki.github.io/media/20200425_%E5%AD%A6%E9%99%A2%E8%AA%AC%E6%98%8E%E4%BC%9A_%E9%87%91%E5%B4%8E%E7%A0%94.pdf">「金崎研究室の説明」</a> <a href="https://kanezaki.github.io/media/20200425_Introduction_of_Kanezaki_Laboratory.pdf"> "Introduction of Kanezaki Laboratory" </a> </LI> 
	    <LI>2019.09.26  情報処理学会連続セミナー2019 第3回: AIと歩む未来(2)：画像・映像処理の最前線<a href="https://speakerdeck.com/kanezaki/ji-jie-xue-xi-woyong-ita3ci-yuan-detaren-shi-nituite"> 「機械学習を用いた3次元データ認識について」 </a> </LI> 
	    <LI>2018.10.30  確率場と深層学習に関する第2回CRESTシンポジウム<a href="https://kanezaki.github.io/media/CRESTSeminar20181030_AsakoKanezaki.pdf"> 「深層学習を用いた三次元物体認識」 </a> </LI> 
	    <LI>2018.08.29  <a href="https://kanezaki.github.io/media/Image-to-Image_Translation_GAN.pdf"> 「Image-to-Image Translation用GAN手法のサーベイ」 </a> </LI> 
	    <LI>2018.05.31  第112回 ロボット工学セミナー「ロボットのための画像処理技術」 <a href="https://kanezaki.github.io/media/RobotSeminar20180531_AsakoKanezaki.pdf"> 「3次元物体認識技術」 </a> </LI> 
	    <LI>2016.06.16  第19回 知的センシングセミナー（主催 中京大学大学院情報科学研究科／企画・運営 橋本研究室） <a href="https://kanezaki.github.io/media/ChukyoSeminar20160616_AsakoKanezaki.pdf"> 「Kinect等の色距離センサを用いた点群処理と3D物体認識」 </a> ※Ubuntu 16.04向け </LI> 
	    <LI>2016.06.08  第22回 画像センシングシンポジウムチュートリアル講演会  <a href="https://kanezaki.github.io/media/SSII2016_AsakoKanezaki.pdf"> 「Kinect等の色距離センサを用いた点群処理と3D物体認識ーベーシックな手法と最新動向・ソフトウェアの紹介ー」 </a> ※Ubuntu 14.04向け </LI> 
	    
            <LI>2014.10.04  第25回 コンピュータビジョン勉強会＠関東 <a href="http://www.slideshare.net/kanejaki/cvsaisentan20141004-kanezaki">「RGBD画像処理と三次元物体認識」</a></LI>

	    <LI>2014.06.18  講義資料 <a href="http://www.mi.t.u-tokyo.ac.jp/kanezaki/pdf/3D_and_weaklearning.pdf">「3D物体認識と弱教師付き学習」</a></LI>
	  </UL>

	  
	  <H2>Book chapters</H2>
	  <OL>
	    <LI><U>金崎朝子</U>，秋月秀一，千葉直也.
	      <BR>『詳解 3次元点群処理 Pythonによる基礎アルゴリズムの実装 (KS理工学専門書)』
	      <BR>講談社，2022年10月発行.
	      <BR><a href="https://www.amazon.co.jp/%E8%A9%B3%E8%A7%A3-3%E6%AC%A1%E5%85%83%E7%82%B9%E7%BE%A4%E5%87%A6%E7%90%86-Python%E3%81%AB%E3%82%88%E3%82%8B%E5%9F%BA%E7%A4%8E%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0%E3%81%AE%E5%AE%9F%E8%A3%85-KS%E7%90%86%E5%B7%A5%E5%AD%A6%E5%B0%82%E9%96%80%E6%9B%B8-%E9%87%91%E5%B4%8E/dp/406529343X/?_encoding=UTF8&pd_rd_w=rKXEn&content-id=amzn1.sym.79ec8dbb-a6ed-4b2c-b8a2-e2dc98848cba&pf_rd_p=79ec8dbb-a6ed-4b2c-b8a2-e2dc98848cba&pf_rd_r=VEHX8P7YH5CV32BYK1P4&pd_rd_wg=xkb0h&pd_rd_r=7f742698-b561-4161-a452-e938c2f2d199&ref_=pd_gw_ci_mcx_mr_hp_atf_m">LINK</a>
	      <BR><BR>

	    <LI><U>Asako Kanezaki</U>, Ryohei Kuga, Yusuke Sugano, and Yasuyuki Matsushita (Chapter authors).
	      <BR>Chapter 2: "Multimodal Scene Understanding: Algorithms, Applications and Deep Learning."
	      <BR>Elsevier, August, 2019. (525 pages)
	      <BR><a href="https://www.elsevier.com/books/multimodal-scene-understanding/yang/978-0-12-817358-9">LINK</a>
	      <BR><BR>

	    <LI>米谷竜，斎藤英雄，池畑諭，牛久祥孝，内山英昭，内海ゆづ子，小野峻佑，片岡裕雄，<U>金崎朝子</U>，川西康友，齋藤真樹，櫻田健，高橋康輔，松井勇佑.
	      <BR>未来へつなぐデジタルシリーズ（37巻）『コンピュータビジョン：広がる要素技術と応用』
	      <BR>共立出版，2018年6月発行.
	      <BR><a href="https://www.amazon.co.jp/%E3%82%B3%E3%83%B3%E3%83%94%E3%83%A5%E3%83%BC%E3%82%BF%E3%83%93%E3%82%B8%E3%83%A7%E3%83%B3-%E5%BA%83%E3%81%8C%E3%82%8B%E8%A6%81%E7%B4%A0%E6%8A%80%E8%A1%93%E3%81%A8%E5%BF%9C%E7%94%A8-%E7%B1%B3%E8%B0%B7-%E7%AB%9C/dp/4320123573/">LINK</a>
	      <BR>
	  </OL>

	  <H2>Journal publications</H2>
	  <OL>
	    <LI>Leyuan Sun, <U>Asako Kanezaki</U>, Guillaume Caron, and Yusuke Yoshiyasu.
	      <BR>Enhancing multimodal-input object goal navigation by leveraging large language models for inferring room-object relationship knowledge.
	      <BR><I>Advanced Engineering Informatics</I>, Volume 65, Part A, 2025.
	      <BR><a href="https://doi.org/10.1016/j.aei.2025.103135">https://doi.org/10.1016/j.aei.2025.103135</a>
	      &nbsp;
	      <BR><a href="https://sunleyuan.github.io/ObjectNav/">Project</a>
	      <BR><BR>

	    <LI>Kei Ota, Devesh K. Jha, and <U>Asako Kanezaki</U>.
	      <BR>A Framework for Training Larger Networks for Deep Reinforcement Learning.
	      <BR><I>Machine Learning</I>, Springer, 2024.
	      <BR><a href="https://doi.org/10.1007/s10994-024-06547-6">https://doi.org/10.1007/s10994-024-06547-6</a>
	      <BR><BR>

	    <LI>Yuzhe Hao, <U>Asako Kanezaki</U>, Ikuro Sato, Rei Kawakami, and Koichi Shinoda.
	      <BR>Egocentric Human Activities Recognition with Multi-modal Interaction Sensing.
	      <BR><I>IEEE Sensors</I>, accepted, 2024.
	      <BR><BR>

	    <LI>Haruyuki Nakagawa and <U>Asako Kanezaki</U>.
	      <BR>Multi-Agent Visual Coordination using Optical Wireless Communication.
	      <BR><I>IEEE Robotics and Automation Letters (RA-L)</I>, accepted, 2023.
	      <BR><a href="https://doi.org/10.1109/LRA.2023.3304905">https://doi.org/10.1109/LRA.2023.3304905</a>
	      <BR><BR>

	    <LI>Hao Zheng, Qiang Zhang, and <U>Asako Kanezaki</U>.
	      <BR>DAC: Disentanglement-and-Calibration Module for Cross-Domain Few-Shot Classification.
	      <BR><I>IEEE Access</I>, Vol. 11, No. 23552466, pp. 82665-82673, 2023.
	      <BR><a href="https://doi.org/10.1109/ACCESS.2023.3294984">https://doi.org/10.1109/ACCESS.2023.3294984</a>
	      <BR><BR>

	    <LI>太田佳，<U>金崎朝子</U>.
	      <BR>動的環境下におけるロボットの動作生成.
	      <BR><I>日本ロボット学会誌</I>, Vol.39, No.7, pp. 581-586, 2021.
	      <BR><BR>

	    <LI><U>Asako Kanezaki</U>, Yasuyuki Matsushita, and Yoshifumi Nishida.
	      <BR>RotationNet for Joint Object Categorization and Unsupervised Pose Estimation from Multi-view Images.
	      <BR><I>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</I>, Vol.43, Issue 1, pp. 269-283, 2021.
	      <BR><a href="https://doi.org/10.1109/TPAMI.2019.2922640">https://doi.org/10.1109/TPAMI.2019.2922640</a>
	      &nbsp;
	      <BR><a href="https://kanezaki.github.io/rotationnet/">Project</a>
	      <BR><BR>

	    <LI>Wonjik Kim*, <U>Asako Kanezaki</U>*, and Masayuki Tanaka.
	      <BR>Unsupervised Learning of Image Segmentation Based on Differentiable Feature Clustering.
	      <BR><I>IEEE Transactions on Image Processing</I>, pp. 8055-8068, 2020.
	      <BR><b>*equal contributions</b>
	      <BR><a href="https://kanezaki.github.io/pytorch-unsupervised-segmentation-tip/">Project</a>
	      <BR><BR>
	    </LI>
	    
	    <LI>Raphael Druon, Yusuke Yoshiyasu, <U>Asako Kanezaki</U>, and Alassane Watt.
	      <BR>Visual Object Search by Learning Spatial Context.
	      <BR><I>IEEE Robotics and Automation Letters (RA-L)</I>, Vol.5, Issue 2, pp. 1279-1286, 2020. (<b>presented in ICRA'20</b>)
	      <BR><a href="https://doi.org/10.1109/LRA.2020.2967677">https://doi.org/10.1109/LRA.2020.2967677</a>
	      <BR><BR>

	    <LI><U>Asako Kanezaki</U>, Jirou Nitta, and Yoko Sasaki.
	      <BR>GOSELO: Goal-Directed Obstacle and Self-Location Map for Robot Navigation using Reactive Neural Networks.
	      <BR><I>IEEE Robotics and Automation Letters (RA-L)</I>, Vol.3, Issue 2, pp. 696-703, 2018. (<b>presented in ICRA'18</b>)
	      <BR><a href="https://doi.org/10.1109/LRA.2017.2783400">https://doi.org/10.1109/LRA.2017.2783400</a>
	      &nbsp;
	      <BR><a href="https://kanezaki.github.io/goselo/">Project</a>
	      <BR><BR>

	    <LI>Nevrez Imamoglu, Wataru Shimoda, Chi Zhang, Yuming Fang, <U>Asako Kanezaki</U>, Keiji Yanai, and Yoshifumi Nishida.
	      <BR>An Integration of Bottom-up and Top-Down Salient Cues on RGB-D Data: Saliency from Objectness vs. Non-Objectness.
	      <BR><I>Signal, Image and Video Processing (Springer)</I>, Vol.12, Issue 2, pp. 307-314, 2017.
	      <BR><a href="https://doi.org/10.1007/s11760-017-1159-7">https://doi.org/10.1007/s11760-017-1159-7</a>
	      <BR><BR>

	    <LI>Zoltan-Csaba Marton, Ferenc Balint-Benczedi, Oscar Martinez Mozos, Nico Blodow, <U>Asako Kanezaki</U>, Lucian Cosmin Goron, Dejan Pangercic, and Michael Beetz.
	      <BR>Part-Based Geometric Categorization and Object Reconstruction in Cluttered Table-Top Scenes. 
	      <BR><I>Journal of Intelligent & Robotic Systems</I>, pp. 35-56, 2014. 
	      <BR><a href="http://dx.doi.org/10.1007/s10846-013-0011-8">http://dx.doi.org/10.1007/s10846-013-0011-8</a>
	      <BR><BR>

	    <LI><U>Asako Kanezaki</U>, Tatsuya Harada, and Yasuo Kuniyoshi.
	      <BR>Partial Matching of Real Textured 3D Objects Using Color Cubic Higher-order Local Auto-Correlation Features. 
	      <BR><I>The Visual Computer</I>, Vol.26, No.10, pp. 1269-1281, 2010.
	      <BR><a href="https://doi.org/10.1007/s00371-010-0521-3">https://doi.org/10.1007/s00371-010-0521-3</a>
	      <BR><BR>

	    <LI>原田達也，<U>金崎朝子</U>，國吉康夫.
	      <BR>三次元環境地図からの物体探索タスク応用を目指したカラー立体高次局所自己相関特徴の開発.
	      <BR><I>日本ロボット学会誌</I>, Vol.27, No.7, pp. 749-758, 2009.
	      <BR>
    
	  </OL>

	  <H2>International Conference</H2>
	  <OL>
	    <LI>Masaru Yajima, Kei Ota, <U>Asako Kanezaki</U>, and Rei Kawakami.
	      <BR>Zero-Shot Peg Insertion: Identifying Mating Holes and Estimating SE(2) Poses with Vision-Language Models.
	      <BR><I>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</I>, accepted, 2025.
	      <BR><a href="https://arxiv.org/abs/2503.06026">Paper</a>
	      <BR><BR>

	    <LI>Kuanting Wu, Kei Ota, and <U>Asako Kanezaki</U>.
	      <BR>FlowLoss: Dynamic Flow-Conditioned Loss Strategy for Video Diffusion Models.
	      <BR><I>The 19th International Conference on Machine Vision Applications (MVA)</I>, accepted, 2025.
	      <BR><a href="https://arxiv.org/abs/2504.14535">Paper</a>
	      <BR><BR>

	    <LI>Jiawei Jiang, Kei Ota, Devesh K. Jha, and <U>Asako Kanezaki</U>.
	      <BR>Modality Selection and Skill Segmentation via Cross-Modality Attention.
	      <BR><I>The 19th International Conference on Machine Vision Applications (MVA)</I>, accepted, 2025.
	      <BR><a href="https://arxiv.org/abs/2504.14573">Paper</a>
	      <BR><BR>

	    <LI>Kohei Matsumoto and <U>Asako Kanezaki</U>.
	      <BR>EED: Embodied Environment Description through Robotic Visual Exploration.
	      <BR><I>The 6th Embodied AI workshop at CVPR</I>, accepted, 2025.
	      <BR><BR>

	    <LI>Yuchen Che, Ryo Furukawa, and <U>Asako Kanezaki</U>.
	      <BR>OP-Align: Object-level and Part-level Alignment for Self-supervised Category-level Articulated Object Pose Estimation.
	      <BR><I>The 18th European Conference on Computer Vision (ECCV)</I>, accepted (oral), 2024.
	      <BR><a href="https://arxiv.org/abs/2408.16547">Paper</a>
	      &nbsp;
	      <a href="https://github.com/YC-Che/OP-Align">Code</a>
	      <BR><BR>

	    <LI>Yunyi Guan and <U>Asako Kanezaki</U>.
	      <BR>Active Object Recognition with Trained Multi-view Based 3D Object Recognition Network.
	      <BR><I>2nd IEEE International Conference on Robotics and Automation (ICRA) Workshop on Mobile Manipulation and Embodied Intelligence</I>, 2024.
	      <BR><a href="https://openreview.net/forum?id=RVeD8k5shC&noteId=RVeD8k5shC">Paper</a>
	      <BR><BR>

	    <LI>Carlos Gutierrez-Alvarez, Rafael Flor-Rodr&iacuteguez-Rabad&aacuten, <U>Asako Kanezaki</U>, and Roberto Javier L&oacutepez-Sastre.
	      <BR>OffNav: Offline Reinforcement Learning for Visual Semantic Navigation.
	      <BR><I>IEEE International Conference on Robotics and Automation (ICRA) Workshop on Human-aligned Reinforcement Learning for Autonomous Agents and Robots</I>, 2024.
	      <BR><a href="https://harlworkshop.github.io/2024/submission_10.pdf">Paper</a>
	      <BR><BR>

	    <LI>Kei Ota, Devesh Jha, Krishna Murthy Jatavallabhula, <U>Asako Kanezaki</U>, and Joshua B. Tenenbaum.
	      <BR>Tactile Estimation of Extrinsic Contact Patch for Stable Placement.
	      <BR><I>IEEE International Conference on Robotics and Automation (ICRA)</I>, accepted, 2024.
	      <BR><a href="https://arxiv.org/abs/2309.14552">Paper</a>
	      <BR><BR>

	    <LI>Haruyuki Nakagawa, Yoshitaka Miyatani, and <U>Asako Kanezaki</U>.
	      <BR>Linking Vision and Multi-Agent Communication through Visible Light Communication using Event Cameras.
	      <BR><I>Int. Joint Conf. on Autonomous Agents & Multiagent Systems (AAMAS)</I>, accepted, 2024.
	      <BR><a href="https://arxiv.org/abs/2402.05619">Paper</a>
	      <BR><BR>

	    <LI>Haru Kondoh and <U>Asako Kanezaki</U>.
	      <BR>Multi-goal Audio-visual Navigation using Sound Direction Map.
	      <BR><I>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</I>, accepted, 2023.
	      <BR><a href="https://arxiv.org/abs/2308.00219">Paper</a>
	      <BR><BR>

	    <LI>Nanami Kotani and <U>Asako Kanezaki</U>.
	      <BR>Point Anywhere: Directed Object Estimation from Omnidirectional Images.
	      <BR><I>Special Interest Group on Computer Graphics and Interactive Techniques Conference Posters (SIGGRAPH '23 Posters)</I>, accepted, 2023.
	      <BR><a href="https://arxiv.org/abs/2308.01010">Paper</a>
	      &nbsp;
	      <a href="https://github.com/NKotani/PointAnywhere">Dataset</a>
	      <BR><BR>

	    <LI>Wenru Zheng, Ryota Yoshihashi, Rei Kawakami, Ikuro Sato, and <U>Asako Kanezaki</U>.
	      <BR>Multi Event Localization by Audio-Visual Fusion with Omnidirectional Camera and Microphone Array.
	      <BR><I>6th CVPR Workshop on Multimodal Learning and Applications (MULA)</I>, accepted, 2023.
	      <BR><a href="https://github.com/zwr17/MLMC-AVE">Dataset</a>
	      <BR><BR>

	    <LI>Hao Zheng, Runqi Wang, Jianzhuang Liu, and <U>Asako Kanezaki</U>.
	      <BR>Cross-Level Distillation and Feature Denoising for Cross-Domain Few-Shot Classification.
	      <BR><I>International Conference on Learning Representations (ICLR)</I>, accepted, 2023.
	      <BR><a href="https://openreview.net/forum?id=Kn-HA8DFik">Paper</a>
	      &nbsp;
	      <a href="https://github.com/jarucezh/cldfd">Code</a>
	      <BR><BR>

	    <LI>Kei Ota, Hsiao-Yu Tung, Kevin A. Smith, Anoop Cherian, Tim K. Marks, Alan Sullivan, <U>Asako Kanezaki</U>, and Joshua B. Tenenbaum.
	      <BR>H-SAUR: Hypothesize, Simulate, Act, Update, and Repeat for Understanding Object Articulations from Interactions.
	      <BR><I>IEEE International Conference on Robotics and Automation (ICRA)</I>, accepted, 2023.
	      <BR><a href="https://arxiv.org/abs/2210.12521">Paper</a>
	      <BR><BR>

	    <LI>Yuzhe Hao, Kuniaki Uto, <U>Asako Kanezaki</U>, Ikuro Sato, Rei Kawakami, and Koichi Shinoda.
	      <BR>EvIs-Kitchen: Egocentric Human Activities Recognition with Video and Inertial Sensor data.
	      <BR><I>Multimedia Modeling (MMM)</I>, accepted, 2023.
	      <BR><BR>

	    <LI>Hana Hoshino, Kei Ota, <U>Asako Kanezaki</U>, and Rio Yokota.
	      <BR>OPIRL: Sample Efficient Off-Policy Inverse Reinforcement Learning via Distribution Matching.
	      <BR><I>IEEE International Conference on Robotics and Automation (ICRA)</I>, accepted, 2022.
	      <BR><a href="https://arxiv.org/abs/2109.04307">Paper</a>
	      &nbsp;
	      <a href="https://github.com/sff1019/opirl">Code</a>
	      <BR><BR>

	    <LI>Rui Fukushima, Kei Ota, <U>Asako Kanezaki</U>, Yoko Sasaki, and Yusuke Yoshiyasu.
	      <BR>Object Memory Transformer for Object Goal Navigation.
	      <BR><I>IEEE International Conference on Robotics and Automation (ICRA)</I>, accepted, 2022.
	      <BR><a href="https://arxiv.org/abs/2203.14708">Paper</a>
	      &nbsp;
	      <a href="https://github.com/TetsuyaMurata/target-driven-navigation-based-on-transformer">Code</a>
	      <BR><BR>

	    <LI>Keisuke Okumura, Ryo Yonetani, Mai Nishimura, and <U>Asako Kanezaki</U>.
	      <BR>CTRMs: Learning to Construct Cooperative Timed Roadmaps for Multi-agent Path Planning in Continuous Spaces.
	      <BR><I>Int. Joint Conf. on Autonomous Agents & Multiagent Systems (AAMAS)</I>, accepted, 2022.
	      <BR><a href="https://omron-sinicx.github.io/ctrm/">Project</a>
	      &nbsp;
	      <a href="https://arxiv.org/abs/2201.09467">Paper</a>
	      &nbsp;
	      <a href="https://github.com/omron-sinicx/ctrm/">Code</a>
	      <BR><BR>

	    <LI>Ryo Yonetani, Tatsunori Taniai, Mohammadamin Barekatain, Mai Nishimura, and <U>Asako Kanezaki</U>.
	      <BR>Path Planning using Neural A* Search.
	      <BR><I>International Conference on Machine Learning (ICML)</I>, pp. 12029-12039, 2021.
	      <BR><a href="https://omron-sinicx.github.io/neural-astar/">Project</a>
	      &nbsp;
	      <a href="https://arxiv.org/abs/2009.07476">Paper</a>
	      <BR><BR>

	    <LI>Takashi Konno, Ayako Amma, and <U>Asako Kanezaki</U>.
	      <BR>Incremental Multi-view Object Detection from a Moving Camera.
	      <BR><I>ACM Multimedia Asia (ACMMMA)</I>, pp. 1-7, 2020.
	      <BR><a href="https://www.ak.c.titech.ac.jp/projects/IMOD/">Project</a>
	      <BR><BR>

	    <LI>Kei Ota, Devesh K. Jha, Tadashi Onishi, <U>Asako Kanezaki</U>, Yusuke Yoshiyasu, Yoko Sasaki, Toshisada Mariyama, Daniel Nikovski.
	      <BR>Deep Reactive Planning in Dynamic Environments.
	      <BR><I>The Conference on Robot Learning (CoRL)</I>, 2020.
	      <BR><a href="https://arxiv.org/abs/2011.00155">Paper</a>
	      &nbsp;
	      <a href="https://www.youtube.com/watch?v=hE-Ew59GRPQ&feature=youtu.be">YouTube</a>
	      <BR><BR>

	    <LI>Kei Ota, Yoko Sasaki, Devesh K. Jha, Yusuke Yoshiyasu, and <U>Asako Kanezaki</U>.
	      <BR>Efficient Exploration in Constrained Environments with Goal-Oriented Reference Path.
	      <BR><I>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2020)</I>, pp. 6061-6068, 2020.
	      <BR><a href="https://arxiv.org/abs/2003.01641">Paper</a>
	      <BR><BR>

	    <LI>Yoko Sasaki, Syusuke Matsuo, <U>Asako Kanezaki</U>, and Hiroshi Takemura.
	      <BR>A3C Based Motion Learning for an Autonomous Mobile Robot in Crowds.
	      <BR><I>IEEE International Conference on System Man and Cybernetics (SMC2019)</I>, pp. 1046-1052, 2019.
	      <BR><BR>

	    <LI>Nevrez Imamoglu, Guanqun Ding, Yuming Fang, <U>Asako Kanezaki</U>, Toru Kouyama, and Ryosuke Nakamura.
	      <BR>Salient object detection on hyperspectral images using features learned from unsupervised segmentation task.
	      <BR><I>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</I>, pp. 2192-2196, 2019.
	      <BR><a href="https://arxiv.org/abs/1902.10993">Paper</a>
	      <BR><BR>

	    <LI><U>Asako Kanezaki</U>, Yasuyuki Matsushita, and Yoshifumi Nishida.
	      <BR>RotationNet: Joint Object Categorization and Pose Estimation Using Multiviews from Unsupervised Viewpoints.
	      <BR><I>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</I>, pp. 5010-5019, 2018.
	      <BR><a href="https://kanezaki.github.io/rotationnet/">Project</a>
	      <BR><BR>

	    <LI><U>Asako Kanezaki</U>.
	      <BR>Unsupervised Image Segmentation by Backpropagation.
	      <BR><I>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</I>, pp. 1543-1547, 2018.
	      <BR><a href="https://kanezaki.github.io/pytorch-unsupervised-segmentation-tip/">Project</a>
	      <BR><BR>

	    <LI>Mikiko Oono, Yoshifumi Nishida, Koji Kitamura, <U>Asako Kanezaki</U>, and Tatsuhiro Yamanaka.
	      <BR>"Change the changeable" framework for implementation research in health.
	      <BR><I>the 10th International Conference on Computer Supported Education (CSEDU)</I>, (oral), 2017.
	      <BR><BR>

	    <LI>Ryohei Kuga, <U>Asako Kanezaki</U>, Masaki Samejima, Yusuke Sugano, and Yasuyuki Matsushita.
	      <BR>Multi-task Learning Using Multi-modal Encoder-Decoder Networks with Shared Skip Connections.
	      <BR><I>IEEE/ISPRS 4th Joint Workshop on Multi-Sensor Fusion for Dynamic Scene Understanding</I>, pp. 403-411, (oral), 2017.
	      <BR><a href="https://doi.org/10.1109/ICCVW.2017.54">https://doi.org/10.1109/ICCVW.2017.54</a>
	      &nbsp;
	      <BR><a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w6/Kuga_Multi-Task_Learning_Using_ICCV_2017_paper.pdf">Paper</a>
	      <BR><BR>

	    <LI>Manolis Savva, Fisher Yu, Hao Su, <U>Asako Kanezaki</U>, Takahiko Furuya, Ryutarou Ohbuchi, Zhichao Zhou, Rui Yu, Song Bai, Xiang Bai, Masaki Aono, Atsushi Tatsuma, S. Thermos, A. Axenopoulos, G. Th. Papadopoulos, P. Daras, Xiao Deng, Zhouhui Lian, Bo Li, Henry Johan, Yijuan Lu, and Sanjeev Mk.
	      <BR>SHREC'17 Track: Large-Scale 3D Shape Retrieval from ShapeNet Core55.
	      <BR><I>Eurographics Workshop on 3D Object Retrieval 2017</I>, 2017.
	      <BR><a href="https://shapenet.cs.stanford.edu/shrec17/shrec17shapenet.pdf">Paper</a>
	      <BR><BR>

	    <LI>Binh-Son Hua, Quang-Trung Truong, Minh-Khoi Tran, Quang-Hieu Pham, <U>Asako Kanezaki</U>, Tang Lee, HungYueh Chiang, Winston Hsu, Bo Li, Yijuan Lu, Henry Johan, Shoki Tashiro, Masaki Aono, Minh-Triet Tran, Viet-Khoi Pham, Hai-Dang Nguyen, Vinh-Tiep Nguyen, Quang-Thang Tran, Thuyen V. Phan, Bao Truong, Minh N. Do, Anh-Duc Duong, Lap-Fai Yu, Duc Thanh Nguyen, and Sai-Kit Yeung.
	      <BR>SHREC'17: RGB-D to CAD Retrieval with ObjectNN Dataset.
	      <BR><I>Eurographics Workshop on 3D Object Retrieval 2017</I>, 2017.
	      <BR><a href="http://sonhua.github.io/pdf/hua-shrec17.pdf">Paper</a>
	      <BR><BR>

	    <LI>Tomoaki Saito, <U>Asako Kanezaki</U>, and Tatsuya Harada.
	      <BR>IBC127: Video Dataset for Fine-grained Bird Classification.
	      <BR><I>IEEE International Conference on Multimedia & Expo (ICME)</I>, 2016.
	      <BR><a href="http://www.mi.t.u-tokyo.ac.jp/projects/IBC127/">Dataset</a>
	      <BR><BR>

	    <LI>Katsunori Ohnishi, Atsushi Kanehira, <U>Asako Kanezaki</U>, and Tatsuya Harada.
	      <BR>Recognizing Activities of Daily Living with a Wrist-mounted Camera.
	      <BR><I>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</I>, (spotlight), 2016.
	      <BR><a href="http://www.mi.t.u-tokyo.ac.jp/static/projects/miladl/">Project</a>
	      <BR><BR>

	    <LI>Chie Kamada, <U>Asako Kanezaki</U>, and Tatsuya Harada.
	      <BR>Probabilistic Semi-Canonical Correlation Analysis.
	      <BR><I>the 23rd Annual ACM International Conference on Multimedia (ACMMM 2015)</I>, pp. 1131-1134, 2015.
	      <BR><BR>

	    <LI><U>Asako Kanezaki</U> and Tatsuya Harada.
	      <BR>3D Selective Search for Obtaining Object Candidates.
	      <BR><I>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2015)</I>, pp. 82-87, 2015.
	      <BR><a href="http://www.mi.t.u-tokyo.ac.jp/kanezaki/pdf/IROS2015_kanezaki.pdf">Paper</a>
	      &nbsp;
	      <a href="https://github.com/kanezaki/selective_search_3d">Code</a>
	      <BR><BR>

	    <LI><U>Asako Kanezaki</U>, Emanuele Rodol&agrave, Daniel Cremers, and Tatsuya Harada.
	      <BR>Learning Similarities for Rigid and Non-Rigid Object Detection.
	      <BR><I>International Conference on 3D Vision (3DV 2014)</I>, pp. 720-727, 2014.
	      <BR><a href="https://vision.in.tum.de/_media/spezial/bib/rodola-3dv14.pdf">Paper</a>
	      <BR><BR>

	    <LI>Sho Inaba, <U>Asako Kanezaki</U>, and Tatsuya Harada.
	      <BR>Automatic Image Synthesis from Keywords Using Scene Context.
	      <BR><I>the 22nd Annual ACM International Conference on Multimedia (ACMMM 2014)</I>, pp. 1149-1152, 2014.
	      <BR><BR>

	    <LI>Masaru Mizuochi, <U>Asako Kanezaki</U>, and Tatsuya Harada.
	      <BR>Clothing Retrieval Based on Local Similarity with Multiple Images.
	      <BR><I>the 22nd Annual ACM International Conference on Multimedia (ACMMM 2014)</I>, pp. 1165-1168, 2014.
	      <BR><BR>

	    <LI><U>Asako Kanezaki</U>, Yusuke Mukuta, and Tatsuya Harada.
	      <BR>Mirror Reflection Invariant HOG descriptors for Object Detection.
	      <BR><I>IEEE 21st International Conference on Image Processing (ICIP 2014)</I>, pp. 1594-1598, 2014.
	      <BR><a href="https://ieeexplore.ieee.org/abstract/document/7025319/">Paper</a>
	      <BR><BR>

	    <LI><U>Asako Kanezaki</U>, Sho Inaba, Yoshitaka Ushiku, Yuya Yamashita, Hiroshi Muraoka, Yasuo Kuniyoshi, and Tatsuya Harada.
	      <BR>Hard Negative Classes for Multiple Object Detection.
	      <BR><I>IEEE International Conference on Robotics and Automation (ICRA 2014)</I>, pp. 3066-3073, 2014.
	      <BR><a href="https://ieeexplore.ieee.org/document/6907300/">Paper</a>
	      <BR><BR>

	    <LI><U>Asako Kanezaki</U>, Yasuo Kuniyoshi, and Tatsuya Harada.
	      <BR>Weakly-supervised Multi-class Object Detection Using Multi-type 3D Features.
	      <BR><I>the 21st Annual ACM International Conference on Multimedia (ACMMM 2013)</I>, pp. 605-608, 2013.
	      <BR><a href="http://www.isi.imi.i.u-tokyo.ac.jp/~harada/pdf/acmmm2013_kanezaki_harada.pdf">Paper</a>
	      &nbsp;
	      <a href="http://www.mi.t.u-tokyo.ac.jp/kanezaki/color_depth_dataset_100.html">Dataset</a>
	      <BR><BR>

	    <LI><U>Asako Kanezaki</U>, Tatsuya Harada, and Yasuo Kuniyoshi.
	      <BR>Scale and Rotation Invariant Color Features for Weakly-Supervised Object Learning in 3D Space.
	      <BR><I>IEEE ICCV Workshop on 3D Representation and Recognition (3dRR-11)</I>, 2011.
	      <BR><a href="http://www.isi.imi.i.u-tokyo.ac.jp/~kanezaki/publications/iccv2011_3dRR_kanezaki.pdf">Paper</a>
	      <BR><BR>

	    <LI><U>Asako Kanezaki</U>, Zoltan-Csaba Marton, Dejan Pangercic, Tatsuya Harada, Yasuo Kuniyoshi, and Michael Beetz.
	      <BR>Voxelized Shape and Color Histograms for RGB-D.
	      <BR><I>IEEE IROS Workshop on Active Semantic Perception and Object Search in the Real World (ASP-AVS-11)</I>, 2011.
	      <BR><a href="https://pdfs.semanticscholar.org/3a23/b78ca2875afa0ecea6a7fdbd997c03f71a8f.pdf">Paper</a>
	      <BR><BR>

	    <LI><U>Asako Kanezaki</U>, Takahiro Suzuki, Tatsuya Harada, and Yasuo Kuniyoshi.
	      <BR>Fast Object Detection for Robots in a Cluttered Indoor Environment Using Integral 3D Feature Table. 
	      <BR><I>IEEE International Conference on Robotics and Automation (ICRA 2011)</I>, pp. 4026-4033, 2011.
	      <BR><a href="http://www.isi.imi.i.u-tokyo.ac.jp/~kanezaki/publications/icra2011_kanezaki.pdf">Paper</a>
	      <BR><BR>

	    <LI><U>Asako Kanezaki</U>, Hideki Nakayama, Tatsuya Harada, and Yasuo Kuniyoshi.
	      <BR>High-speed 3D Object Recognition Using Additive Features in A linear Subspace. 
	      <BR><I>2010 IEEE International Conference on Robotics and Automation (ICRA 2010)</I>, pp. 3128-3134, 2010.
	      <BR><font color="#ff0000">2010 IEEE Robotics and Automation Society Japan Chapter Young Award (ICRA 2010)</font>
	      <BR><a href="http://www.isi.imi.i.u-tokyo.ac.jp/~kanezaki/publications/icra2010_kanezaki.pdf">Paper</a>
	      <BR><BR>

	    <LI><U>Asako Kanezaki</U>, Tatsuya Harada, and Yasuo Kuniyoshi.
	      <BR>Partial Matching for Real Textured 3D Objects using Color Cubic Higher-order Local Auto-Correlation Features.
	      <BR><I>Eurographics Workshop on 3D Object Retrieval 2009</I>, pp. 9-12, 2009.
	      <BR><BR>

	  </OL>

	  <H2>Domestic Conference</H2>
	  <OL>
	    <LI>近藤 暖，<U>金崎 朝子</U>.
	      <BR>説明文生成を補助タスクとする意味的視聴覚ナビゲーション
	      <BR><I>Meeting on Image Recognition and Understandings (MIRU)</I>, OS-1E-08, 2024.
	      <BR><BR>

	    <LI>齊藤 温人，<U>金崎 朝子</U>.
	      <BR>オープンセット認識におけるクラス間の類似度に基づく学習クラス選択
	      <BR><I>Meeting on Image Recognition and Understandings (MIRU)</I>, IS-1-086, 2024.
	      <BR><BR>

	    <LI>中村 遼平，<U>金崎 朝子</U>.
	      <BR>単一画像からの関節付き物体の新規視点画像生成
	      <BR><I>Meeting on Image Recognition and Understandings (MIRU)</I>, IS-1-109, 2024.
	      <BR><BR>

	    <LI>太田 朔哉，<U>金崎 朝子</U>.
	      <BR>テキスト付き動画データを用いた動作生成に関する研究
	      <BR><I>Meeting on Image Recognition and Understandings (MIRU)</I>, IS-3-083, 2024.
	      <BR><BR>

	    <LI>小谷 七海，<U>金崎 朝子</U>.
	      <BR>注目領域の抽出に基づく全天球画像からの指示物体推定
	      <BR><I>Meeting on Image Recognition and Understandings (MIRU)</I>, IS2-66, 2023.
	      <BR><BR>

	    <LI>合楽 慧，<U>金崎 朝子</U>.
	      <BR>画像の意味的領域分割を用いた強化学習による室内環境探索
	      <BR><I>Meeting on Image Recognition and Understandings (MIRU)</I>, IS2-109, 2023.
	      <BR><BR>

	    <LI>近藤 暖，<U>金崎 朝子</U>.
	      <BR>動的複数音源定位手法を用いたマルチゴール視聴覚ナビゲーション
	      <BR><I>Meeting on Image Recognition and Understandings (MIRU)</I>, IS3-109, 2023.
	      <BR><BR>

	    <LI>Yunyi Guan and <U>Asako Kanezaki</U>.
	      <BR>Active Object Recognition in Discrete and Continuous View Settings
	      <BR><I>Meeting on Image Recognition and Understandings (MIRU)</I>, IS1-111, 2023.
	      <BR><BR>

	    <LI>神山 慶伍，太田 佳，高波 亮介，<U>金崎 朝子</U>.
	      <BR>強化学習エージェントの軌跡を用いたサンプルベース経路計画
	      <BR><I>Pattern Recognition and Media Understanding (PRMU)</I>, pp. 1-6, 2022.
	      <BR><BR>

	    <LI>中川 遥之，<U>金崎 朝子</U>.
	      <BR>光無線通信を用いたマルチエージェント強化学習
	      <BR><I>Meeting on Image Recognition and Understandings (MIRU)</I>, IS1-37, 2022. 
	      <BR><BR>

	    <LI>熊坂 雅之，<U>金崎 朝子</U>.
	      <BR>セグメンテーション画像を入力としたシーン分類手法の検討
	      <BR><I>Meeting on Image Recognition and Understandings (MIRU)</I>, IS2-32, 2022. 
	      <BR><BR>

	    <LI>北山 翼，川上 玲，佐藤 育郎，吉橋 亮太，<U>金崎 朝子</U>.
	      <BR>球面畳み込みの時間軸拡張による全天球映像からの行動種別の領域分割
	      <BR><I>Meeting on Image Recognition and Understandings (MIRU)</I>, IS1-68, 2022. 
	      <BR><BR>

	    <LI>Wenru Zheng, Rei Kawakami, Ikuro Sato, Ryota Yoshihashi, and <U>Asako Kanezaki</U>.
	      <BR>Event Recognition by Audio-Visual Fusion with Omnidirectional Camera and Microphone Array
	      <BR><I>Meeting on Image Recognition and Understandings (MIRU)</I>, IS2-65, 2022. 
	      <BR><BR>

	    <LI>加納諒也，和田一義，<U>金崎朝子</U>，冨沢哲雄，谷川民生.
	      <BR>深層学習による物体認識のためのマーカの検討 ～第一報：AlexNetによる認識率の比較～
	      <BR><I>日本機械学会ロボティクス・メカトロニクス講演会</I>, 2019. 
	      <BR><BR>

	    <LI>佐々木洋子，松尾修佑，<U>金崎朝子</U>，竹村裕.
	      <BR>歩行者観測履歴を用いた深層強化学習による車輪ロボットの雑踏切り抜け動作生成
	      <BR><I>日本機械学会ロボティクス・メカトロニクス講演会</I>, 2019. 
	      <BR><BR>

	    <LI>佐々木 洋子，坂東 宜昭，<U>金崎 朝子</U>.
	      <BR>科学館での自律移動ロボット実証実験 ～ロボットは人混みで音を聞き分けられるか!?～
	      <BR><I>第52回人工知能学会 AI チャレンジ研究会</I>, 2018. 
	      <BR><BR>

	    <LI>渋谷 薫，<U>金崎 朝子</U>，大西 正輝.
	      <BR>深層学習による画像識別問題に帰着した人の流れのシミュレーション
	      <BR><I>Meeting on Image Recognition and Understandings (MIRU)</I>, 2018. 
	      <BR><BR>

	    <LI><U>金崎 朝子</U>，松下 康之，西田 佳史.
	      <BR>視点教示なし多視点画像深層学習による物体のカテゴリ・姿勢同時推定
	      <BR><I>第24回 画像センシングシンポジウム (SSII)</I>, 2018. 
	      <BR><BR>

	    <LI>藤橋 一輝，木村 雅之，<U>金崎 朝子</U>，小澤 順.
	      <BR>半教師あり学習による商品画像中の個数と位置の同時推定
	      <BR><I>第32回 人工知能学会全国大会 (JSAI)</I>, 2018. 
	      <BR><BR>

	    <LI>鈴木 亮太，QIU YUE，<U>金崎 朝子</U>，岩田 健司，佐藤 雄隆.
	      <BR>物体認識技術を活用したインタラクティブVR空間の構成
	      <BR><I>第23回 画像センシングシンポジウム (SSII)</I>, 2017. 
	      <BR><BR>

	    <LI>西田 佳史，北村 光司，佐々木 洋子，<U>金崎 朝子</U>，Shi Boxin，大野 美喜子，楠本 欣司.
	      <BR>繋げる人工知能 ：生活機能レジリエント社会のための スマートリビングネットを用いた生活知能研究
	      <BR><I>第17回 SICEシステムインテグレーション部門講演会</I>, 2016. 
	      <BR><BR>

	    <LI><U>金崎 朝子</U>.
	      <BR>三次元情報を活用した物体検出の三手法
	      <BR><I>Vision Enginieering Workshop (ViEW)</I>, OS5-01, pp. 284-287, 2015.
	      <BR><BR>

	    <LI><U>金崎 朝子</U>，Emanuele Rodol&agrave，原田 達也.
	      <BR>グラフマッチング学習を用いたRGB-D画像からの物体検出
	      <BR><I>第20回 ロボティクスシンポジア</I>, pp. 432-437, 2015.
	      <BR><BR>

	    <LI><U>金崎 朝子</U>，Emanuele Rodol&agrave，Daniel Cremers，原田 達也.
	      <BR>対応点集合類似度学習を用いた剛体・非剛体物体検出
	      <BR><I>Pattern Recognition and Media Understanding (PRMU)</I>, pp. 13-18, 2014.
	      <BR><BR>

	    <LI><U>金崎 朝子</U>，Emanuele Rodol&agrave，原田 達也.
	      <BR>RGB-D画像からの物体検出における対応点集合類似度の学習
	      <BR><I>the 32th Annual Conference of the Robotics Society of Japan (RSJ)</I>, 3I2-03, 2014. 
	      <!--BR><font color="#ff0000">2015 Young Investigation Excellence Award, the Robotics Society of Japan</font-->
	      <BR><font color="#ff0000">2015 研究奨励賞．第30回 日本ロボット学会 学術講演会（RSJ）</font>
	      <BR><BR>

	    <LI><U>金崎 朝子</U>，稲葉 翔，牛久 祥孝，山下 裕也，村岡 宏是，原田 達也，國吉 康夫.
	      <BR>大規模画像データセットを用いたマルチクラス物体検出器の同時学習 〜物体毎に特化した負例クラスの導入〜
	      <BR><I>Pattern Recognition and Media Understanding (PRMU)</I>, pp. 105-112, 2012.
	      <BR><font color="#ff0000">2012 研究奨励賞. 電子情報通信学会パターン認識・マルチメディア理解研究会 (PRMU)</font>
	      <BR><BR>

	    <LI><U>金崎 朝子</U>，原田 達也，國吉 康夫.
	      <BR>多種類の三次元特徴量を用いた物体セグメンテーションの弱教師付き学習
	      <BR><I>Meeting on Image Recognition and Understandings (MIRU)</I>, OS14-03, 2012. 
	      <BR><BR>

	    <LI>稲葉 翔，村岡 宏是，山下 裕也，牛久 祥孝，<U>金崎 朝子</U>，原田 達也，國吉 康夫.
	      <BR>学習時間に着目した効率的な大規模画像分類
	      <BR><I>Meeting on Image Recognition and Understandings (MIRU)</I>, OS6-03, 2012. 
	      <BR><BR>

	    <LI><U>金崎 朝子</U>，原田 達也，國吉 康夫.
	      <BR>カラー三次元積分特徴と線形部分空間法による実世界からの高速物体検出
	      <BR><I>Meeting on Image Recognition and Understandings (MIRU)</I>, IS1-59, pp. 448-455, 2010.
	      <BR><BR>

	    <LI><U>金崎 朝子</U>，原田 達也，國吉 康夫.
	      <BR>積分特徴と部分空間法を用いた高速三次元物体認識の実データへの適用 
	      <BR><I>Pattern Recognition and Media Understanding (PRMU)</I>, pp. 207-212, 2009.
	      <BR><BR>

	    <LI><U>金崎 朝子</U>，中山 英樹，原田 達也，國吉 康夫.
	      <BR>部分空間法とカラー立体高次局所自己相関特徴を用いた高速三次元物体認識
	      <BR><I>Meeting on Image Recognition and Understandings (MIRU)</I>, OS4-3:103-OS4-3:110, 2009. 
	      <BR><BR>

	    <LI><U>金崎 朝子</U>，原田 達也，國吉 康夫.
	      <BR>三次元環境地図からの物体探索タスク応用を目指したカラー立体高次局所自己相関特徴の開発
	      <BR><I>the 26th Annual Conference of the Robotics Society of Japan (RSJ)</I>, 1L3-06, 2008. 
	      <BR><BR>

	  </OL>

	  <H2>Awards</H2>
	  <UL>
	    <LI>2020 日本機械学会奨励賞（研究）．
	    <LI>2019 論文評価貢献賞. 第22回 画像の認識・理解シンポジウム（MIRU2019）
	    <LI>2017 <U>First place</U> in Task 1 in SHREC 2017: Large-scale 3D Shape Retrieval from ShapeNet Core55 Challenge.
	    <LI>2017 <U>First place</U> in SHREC 2017: RGB-D Object-to-CAD Retrieval Contest.
	    <LI>2015 研究奨励賞．第30回 日本ロボット学会 学術講演会（RSJ）
	    <LI>2014 船井研究奨励賞.
	    <LI>2012 研究奨励賞. 電子情報通信学会パターン認識・マルチメディア理解研究会 (PRMU)
	    <LI>2011 <U>Third place</U> in the classification task, <U>second place</U> in the detection task. Large Scale Visual Recognition Challenge 2011 (ILSVRC2011)
	    <LI>2010 IEEE Robotics and Automation Society Japan Chapter Young Award (ICRA 2010)
	  </UL>
	  	  
	  <H2>Grants/Fellowships</H2>
	  <UL>
	    <LI>2021-2028 JST 創発的研究支援事業
	    <LI>2020 栢森情報科学振興財団 研究助成金
	    <LI>2020-2023 日本学術振興会 (JSPS) 若手研究
	    <LI>2016-2019 日本学術振興会 (JSPS) 挑戦的萌芽研究
	    <LI>2015 Microsoft Research sponsors collaborative research (CORE) project
	    <LI>2014-2016 日本学術振興会 (JSPS) 研究活動スタート支援
	    <LI>2010-2013 日本学術振興会 (JSPS) 特別研究員 (DC1)
	  </UL>

	  <BR>
	  
	  <H2>Contact</H2>
	  <UL>
	    <LI>e-mail: kanezaki[at]comp.isct.ac.jp
	  </UL>
	  	  
    <!--/TD></TR></TABLE-->
    
</UL>

</div>

<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

</BODY>
</HTML>

